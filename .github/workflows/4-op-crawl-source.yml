name: "4. Op: Crawl Source"

# Crawl all accessible pages under a source URL
on:
  workflow_dispatch:
    inputs:
      source_url:
        description: "Source URL to crawl (defines crawl boundary)"
        required: true
        type: string
      crawl_scope:
        description: "Crawl scope constraint"
        required: false
        type: choice
        options:
          - path
          - host
          - domain
        default: path
      max_pages_per_run:
        description: "Max pages to process this run (for chunking)"
        required: false
        type: number
        default: 1000
      force_restart:
        description: "Restart crawl from scratch (discard existing state)"
        required: false
        type: boolean
        default: false

permissions:
  contents: write
  issues: write

jobs:
  crawl-source:
    runs-on: ubuntu-latest
    timeout-minutes: 300  # 5 hours max for large crawls
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: "pip"
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Run Crawler Agent
        id: crawler
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}
          GITHUB_REPOSITORY: ${{ github.repository }}
        run: |
          python -m main agent run \
            --mission crawl_source \
            --input "source_url=${{ inputs.source_url }}" \
            --input "crawl_scope=${{ inputs.crawl_scope || 'path' }}" \
            --input "max_pages_per_run=${{ inputs.max_pages_per_run || 1000 }}" \
            --input "force_restart=${{ inputs.force_restart || false }}"
      
      - name: Commit crawl progress
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(crawl): update crawl state and content [skip ci]"
          file_pattern: |
            evidence/parsed/**
            knowledge-graph/crawls/**
          skip_dirty_check: false
          skip_fetch: false
          skip_checkout: false
